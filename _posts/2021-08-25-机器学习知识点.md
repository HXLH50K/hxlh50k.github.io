---
layout: post
title: 机器学习知识点
subtitle: 未完成，随缘更新，不含深度学习
date: 2021-08-25
author: hxlh50k
header-img: null
catalog: true
tags:
  - 机器学习
  - 深度学习
---

# Logistic Regression

## 简介

逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达到将数据二分类的目的。

## 目的

二分类

## 优点：

- 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
- 模型效果不错。在工程上是可以接受的（作为 baseline），如果特征工程做的好，效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。
- 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化 SGD 发展比较成熟。
- 方便调整输出结果，通过调整阈值的方式。

## 缺点：

- 准确率欠佳。因为形式非常的简单，而现实中的数据非常复杂，因此，很难达到很高的准确性。
- 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1。我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
- 无法自动的进行特征筛选。
- 只能处理二分类问题。

# SVM

## 基础

- 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
- 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
- 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

## Loss 函数

<!-- prettier-ignore-start -->
$$ L_{SVM}=\sum^N_{i=1}{L_{hinge}(y_i)+λ||w||^2} $$ 
$$ L_{hinge}=[1-y_i (w∙x_i+b)]_+ $$

<!-- prettier-ignore-end -->

## 间隔最大化

当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系。

## 对偶问题

- 对偶问题往往更易求解
- 可以自然引入核函数，进而推广到非线性分类问题

## 核函数

- 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。
- 使用核函数不需要知道具体的映射函数

## 为什么 SVM 对缺失数据敏感

SVM 没有处理缺失值的策略。而 SVM 希望样本在特征空间中线性可分，所以特征空间的好坏对 SVM 的性能很重要。

## 核函数之间的区别

- 线性核  
  主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。
- RBF（高斯）核  
  主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。
