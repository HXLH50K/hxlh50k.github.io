---
layout: post
title: 深度学习基础知识点
subtitle: 未完成，随缘更新
date: 2021-08-25
author: hxlh50k
header-img: null
catalog: true
tags:
  - 机器学习
  - 深度学习
---

# 优化算法

## GD

1. 确定优化模型的假设函数及损失函数。
2. 初始化参数，随机选取取值范围内的任意数；
3. 迭代操作：
   1. 计算当前梯度
   2. 修改新的变量
   3. 计算朝最陡的下坡方向走一步
   4. 判断是否需要终止，如否，梯度更新
   5. 得到全局最优解或者接近全局最优解。

## BGD 批量梯度下降法

1. 采用所有数据来梯度下降。
2. 批量梯度下降法在样本量很大的时候，训练速度慢。

## SGD 随机梯度下降法

1. 随机梯度下降用一个样本来梯度下降。
2. 训练速度很快。
3. 随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。
4. 收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。

## MBGD 小批量梯度下降

前两者折中

## 动量优化算法

- GD 缺点：当接近最优值时梯度会比较小，由于学习率固定，收敛速度会变慢，有时甚至陷入局部最优。
- 动量基本思想：考虑历史梯度，引导参数朝着最优值更快收敛。直观上来说，要是当前时刻的梯度与历史梯度方向趋近，这种趋势会在当前时刻加强，否则当前时刻的梯度方向减弱。如果两个坐标轴梯度差距大，会在大梯度的轴上震荡

## 牛顿动量（Nesterov）算法

不在当前时刻修正梯度，而是预测下一次的位置，进行梯度修正。可以减弱震荡

# 自适应学习率优化算法

## Adagrad

1. 独立设置参数空间每个轴方向上的学习率
2. 参数空间每个方向的学习率反比于 x 的平方根。x=该方向上梯度分量的所有历史平方值之和。 $$ \vec{r}\leftarrow\vec{r}+\hat{\vec{g}}\odot\hat{\vec{g}} $$  
   $$ \vec{\theta}\leftarrow\vec{\theta}-\frac{\epsilon}{\sqrt{\vec{r}}}\odot\hat{\vec{g}} $$

## RMSProp

1. 将 Adagrad 梯度累计策略修改为指数加权的移动平均 $$ \vec{r}\leftarrow\rho\vec{r}+(1-\rho)\hat{\vec{g}}\odot\hat{\vec{g}} $$  
   $$ \vec{\theta}\leftarrow\vec{\theta}-\frac{\epsilon}{\sqrt{\vec{r}}}\odot\hat{\vec{g}} $$

## AdaDelta

1. 将 Adagrad 梯度累计策略修改为只考虑过去窗口 window 内的梯度。
2. RMSProp 可以认为是一种软化的 AdaDelta，衰减速率 $window=\frac{1}{1-ρ}$。

## Adam

1. RMSProp 算法中，通过累计平方梯度（采用指数移动平均）来修正学习率。
2. 而 Adam 算法中，不仅采用同样的方式来修正学习率，还通过累计梯度（采用指数移动平均）来修正梯度。

# batch-size 如何设置

过小：花费时间多，同时梯度震荡严重，不利于收敛过大：不同 batch 的梯度方向没有任何变化，容易陷入局部极小值

# 正则化：

- L1、L2 都是对损失函数的优化，L 范式都是为了防止模型过拟合，所谓范式就是加入参数的约束。
- L1 的作用是为了矩阵稀疏化。假设的是模型的参数取值满足拉普拉斯分布。
- L2 的作用是为了使模型更平滑，得到更好的泛化能力。假设的是参数是满足高斯分布，L2 对大数和异常值更敏感。

# 常见损失函数

<!-- prettier-ignore-start -->
## BCE
$$ L_{BCE}=-\frac{1}{N}\sum^N_{i=1}{y_i\log{p_i}+(1-y_i)\log{(1-p_i)}} $$
$y_i$ 的取值为 $\{0, 1\}$ ，对于每个sample，公式中只有一项生效

## CE
$$ L_{CE}=-\frac{1}{N}\sum^N_{i=1}{\sum^C_{c=1}{y^{(c)}_i\log{p^{(c)}_i}}} $$

## MAE/L1
$$ L_1 = \frac{1}{N}\sum^N_{i=1}{|y_i-p_i|} $$

## Smooth L1
$$ L_{s1} = \begin{cases} 
0.5x^2 &, & |x|<1 \\
|x|-0.5 &, & |x|\geq1 &
\end{cases}$$

## MSE/L2
$$ L_1 = \frac{1}{N}\sum^N_{i=1}{(y_i-p_i)^2} $$

## FOCAL
$$ L_{FOCAL} = \frac{1}{N}\sum^N_{i=1}{\alpha(1-p_i)^\gamma y_i\log{p_i}+(1-\alpha)p_i^\gamma (1-y_i)\log(1-p_i)} , \alpha=0.25,\gamma=2$$

## DICE
$$ L_{Dice} = 1- 2*\frac{|y\cap p|}{|y|+|p|} $$

## IOU
$$ L_{IOU} = 1-\frac{|y \cap p|}{|y \cup p|} $$

# 激活函数

## sigmoid
$$ f(x) = \frac{1}{1+e^{-x}} $$  
$$ f'(x) = f(x)(1-f(x)) $$
问题：梯度消失，均值不为0  

## hard sigmoid
$$ f(x) = \begin{cases}
0 &,&{x<-2.5}   \\
0.2x-2.5 &,&{-2.5\leq x \leq 2.5}  \\
1 &,&{x>2.5}
\end{cases}$$

## softmax
$$ f(x) = \frac{1}{N}\sum^N_{i=1}{\frac{e^{x_i}}{\sum^{C}_{c=1}{e^{x_i^{(c)}}}}} $$

## tanh
$$ f(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} $$
解决：均值为0
问题：梯度消失

## ReLU
$$ f(x)=\max⁡(x,0) $$
解决：梯度消失
问题：死区，无穷大

## ReLU6
$$ f(x)=\min⁡(\max⁡(x,0),6)$$
解决：无穷大
问题：死区

## Leaky ReLU
$$ f(x) = \max(x,\alpha x),usually0<\alpha \ll 1 $$
解决：死区
问题：α不好设置

## PReLU
α可学习的Leaky ReLU，所有层相同

## RReLU
α可学习的Leaky ReLU，随机均值/高斯初始化，所有层不同

## ELU
$$ f(x) = \begin{cases}
x &,& x\geq 0 \\
\lambda (e^{-x}-1) &,& x<0
\end{cases} $$
缺点：计算量大

## GeLU (for bert)
$$ xP(X\leq x)=xΦ(x) $$
Φ(x)  is Standard normal distribution.

## Maxout：
一种激活函数层

## Swish
$$ f(x)=x\sigma(\beta x)$$
特点：
* 无上界，不会梯度饱和
* 有下界，可以产生更强的正则化效果
* 非单调
* 处处可导
## H-Swish
$$ f(x) = x\frac{ReLU6(x+3)}{6} $$

# 评价指标

## 分类

### 准确率

$$ Accuracy = \frac{TP+TN}{TP+FN+FP+TN}$$

### 精确率
$$ Precision = \frac{TP}{TP+FP}$$

### 召回率
$$ Recall = \frac{TP}{TP+FN} $$

### F-score
$$ F_β = \frac{(1+\beta^2 )TP}{(1+\beta^2 )TP+\beta^2 FN+FP}$$
当beta大于1，更多关注recall；当beta小于1，更多关注precision。

### AUC/GAUC https://zhuanlan.zhihu.com/p/84350940

#### ROC
给定所有样本的正负和预测的概率值，给定阈值集合{0.1,0.2…0.9}，遍历所有阈值，并查看给定每个阈值的情况下的分类情况如何。横坐标TPR，纵坐标FPR

#### AUC
$$ AUC = \frac{area under ROC}{x*y} = \frac{\sum^M_{i=1}{N_i}}{M*N} $$
有M个正样本，N个负样本

#### GAUC
用于广告推荐领域，计算每个用户的auc，然后加权平均，最后得到group auc，这样就能减少不同用户间的排序结果不太好比较这一影响。

<!-- prettier-ignore-end -->
