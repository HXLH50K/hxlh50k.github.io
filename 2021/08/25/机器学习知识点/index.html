<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/favicon/web-app-manifest-192x192.png?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/favicon/web-app-manifest-512x512.png?v=2.8.0" type="image/png" sizes="32x32"><meta name="description" content="未完成，随缘更新，不含深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习知识点">
<meta property="og:url" content="https://hxlh50k.github.io/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/index.html">
<meta property="og:site_name" content="一个博客">
<meta property="og:description" content="未完成，随缘更新，不含深度学习">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-08-24T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-30T13:54:10.754Z">
<meta property="article:author" content="hxlh50k">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary"><title>机器学习知识点 | 一个博客</title><link ref="canonical" href="https://hxlh50k.github.io/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 8.0.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">一个博客</div><div class="header-banner-info__subtitle">逸一时，误一世</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">机器学习知识点</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-08-25</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-09-30</span></span></div></header><div class="post-body">
        <h1 id="Logistic-Regression"   >
          <a href="#Logistic-Regression" class="heading-link"><i class="fas fa-link"></i></a><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1>
      
        <h2 id="简介"   >
          <a href="#简介" class="heading-link"><i class="fas fa-link"></i></a><a href="#简介" class="headerlink" title="简介"></a>简介</h2>
      <p>逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达到将数据二分类的目的。</p>

        <h2 id="目的"   >
          <a href="#目的" class="heading-link"><i class="fas fa-link"></i></a><a href="#目的" class="headerlink" title="目的"></a>目的</h2>
      <p>二分类</p>

        <h2 id="优点："   >
          <a href="#优点：" class="heading-link"><i class="fas fa-link"></i></a><a href="#优点：" class="headerlink" title="优点："></a>优点：</h2>
      <ul>
<li>形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。</li>
<li>模型效果不错。在工程上是可以接受的（作为 baseline），如果特征工程做的好，效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。</li>
<li>训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化 SGD 发展比较成熟。</li>
<li>方便调整输出结果，通过调整阈值的方式。</li>
</ul>

        <h2 id="缺点："   >
          <a href="#缺点：" class="heading-link"><i class="fas fa-link"></i></a><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h2>
      <ul>
<li>准确率欠佳。因为形式非常的简单，而现实中的数据非常复杂，因此，很难达到很高的准确性。</li>
<li>很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1。我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。</li>
<li>无法自动的进行特征筛选。</li>
<li>只能处理二分类问题。</li>
</ul>

        <h1 id="SVM"   >
          <a href="#SVM" class="heading-link"><i class="fas fa-link"></i></a><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1>
      
        <h2 id="基础"   >
          <a href="#基础" class="heading-link"><i class="fas fa-link"></i></a><a href="#基础" class="headerlink" title="基础"></a>基础</h2>
      <ul>
<li>当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；</li>
<li>当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；</li>
<li>当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</li>
</ul>

        <h2 id="Loss-函数"   >
          <a href="#Loss-函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#Loss-函数" class="headerlink" title="Loss 函数"></a>Loss 函数</h2>
      <!-- prettier-ignore-start -->
<p>$$ L_{SVM}&#x3D;\sum^N_{i&#x3D;1}{L_{hinge}(y_i)+λ||w||^2} $$<br>$$ L_{hinge}&#x3D;[1-y_i (w∙x_i+b)]_+ $$</p>
<!-- prettier-ignore-end -->


        <h2 id="间隔最大化"   >
          <a href="#间隔最大化" class="heading-link"><i class="fas fa-link"></i></a><a href="#间隔最大化" class="headerlink" title="间隔最大化"></a>间隔最大化</h2>
      <p>当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系。</p>

        <h2 id="对偶问题"   >
          <a href="#对偶问题" class="heading-link"><i class="fas fa-link"></i></a><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2>
      <ul>
<li>对偶问题往往更易求解</li>
<li>可以自然引入核函数，进而推广到非线性分类问题</li>
</ul>

        <h2 id="核函数"   >
          <a href="#核函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2>
      <ul>
<li>当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。</li>
<li>使用核函数不需要知道具体的映射函数</li>
</ul>

        <h2 id="核函数公式"   >
          <a href="#核函数公式" class="heading-link"><i class="fas fa-link"></i></a><a href="#核函数公式" class="headerlink" title="核函数公式"></a>核函数公式</h2>
      <!-- prettier-ignore-start -->

<ul>
<li>线性核<br>$$ K(x_i,x_j) &#x3D; x_i^Tx_j $$</li>
<li>多项式核<br>$$ K(x_i,x_j) &#x3D; (\gamma x^T_ix_j+b)^d $$</li>
<li>径向基函数核&#x2F;RBF&#x2F;高斯<br>$$ K(x_i,x_j) &#x3D; \exp(-\gamma |x_i-x_j|^2) $$</li>
<li>sigmoid 核<br>$$ K(x_i,x_j) &#x3D; \tanh(\gamma x_i^Tx_j+b) $$</li>
</ul>
<!-- prettier-ignore-end -->


        <h2 id="为什么-SVM-对缺失数据敏感"   >
          <a href="#为什么-SVM-对缺失数据敏感" class="heading-link"><i class="fas fa-link"></i></a><a href="#为什么-SVM-对缺失数据敏感" class="headerlink" title="为什么 SVM 对缺失数据敏感"></a>为什么 SVM 对缺失数据敏感</h2>
      <p>SVM 没有处理缺失值的策略。而 SVM 希望样本在特征空间中线性可分，所以特征空间的好坏对 SVM 的性能很重要。</p>

        <h2 id="核函数之间的区别"   >
          <a href="#核函数之间的区别" class="heading-link"><i class="fas fa-link"></i></a><a href="#核函数之间的区别" class="headerlink" title="核函数之间的区别"></a>核函数之间的区别</h2>
      <ul>
<li>线性核<br>主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。</li>
<li>RBF（高斯）核<br>主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。</li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://hxlh50k.github.io">hxlh50k</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://hxlh50k.github.io/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/">https://hxlh50k.github.io/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://hxlh50k.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://hxlh50k.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/08/27/JS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%89%8B%E5%86%99call,%20apply,%20bind/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">JS学习笔记：手写call, apply, bind</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/08/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9/"><span class="paginator-prev__text">深度学习基础知识点</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Logistic-Regression"><span class="toc-number">1.</span> <span class="toc-text">
          Logistic Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">
          简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84"><span class="toc-number">1.2.</span> <span class="toc-text">
          目的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9%EF%BC%9A"><span class="toc-number">1.3.</span> <span class="toc-text">
          优点：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A"><span class="toc-number">1.4.</span> <span class="toc-text">
          缺点：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SVM"><span class="toc-number">2.</span> <span class="toc-text">
          SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80"><span class="toc-number">2.1.</span> <span class="toc-text">
          基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Loss-%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">
          Loss 函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="toc-number">2.3.</span> <span class="toc-text">
          间隔最大化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-number">2.4.</span> <span class="toc-text">
          对偶问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">2.5.</span> <span class="toc-text">
          核函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E5%85%AC%E5%BC%8F"><span class="toc-number">2.6.</span> <span class="toc-text">
          核函数公式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-SVM-%E5%AF%B9%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE%E6%95%8F%E6%84%9F"><span class="toc-number">2.7.</span> <span class="toc-text">
          为什么 SVM 对缺失数据敏感</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">2.8.</span> <span class="toc-text">
          核函数之间的区别</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/avatar/avatar-512x512.png" alt="avatar"></div><p class="sidebar-ov-author__text">hxlh50k</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">14</div><div class="sidebar-ov-state-item__name">归档</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2025</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>hxlh50k</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v8.0.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.8.0</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>