---
layout: post
title: ［随缘更新］机器学习相关知识点
subtitle: None
date: 2021-08-25
author: hxlh50k
header-img: null
catalog: true
tags:
  - 机器学习
  - 深度学习
---

# 优化算法

## GD

1. 确定优化模型的假设函数及损失函数。
2. 初始化参数，随机选取取值范围内的任意数；
3. 迭代操作：
   1. 计算当前梯度
   2. 修改新的变量
   3. 计算朝最陡的下坡方向走一步
   4. 判断是否需要终止，如否，梯度更新
   5. 得到全局最优解或者接近全局最优解。

## BGD 批量梯度下降法

1. 采用所有数据来梯度下降。
2. 批量梯度下降法在样本量很大的时候，训练速度慢。

## SGD 随机梯度下降法

1. 随机梯度下降用一个样本来梯度下降。
2. 训练速度很快。
3. 随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。
4. 收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。

## MBGD 小批量梯度下降

前两者折中

## 动量优化算法

- GD 缺点：当接近最优值时梯度会比较小，由于学习率固定，收敛速度会变慢，有时甚至陷入局部最优。
- 动量基本思想：考虑历史梯度，引导参数朝着最优值更快收敛。
  直观上来说，要是当前时刻的梯度与历史梯度方向趋近，这种趋势会在当前时刻加强，否则当前时刻的梯度方向减弱。
  如果两个坐标轴梯度差距大，会在大梯度的轴上震荡

## 牛顿动量（Nesterov）算法

不在当前时刻修正梯度，而是预测下一次的位置，进行梯度修正。
可以减弱震荡

# 自适应学习率优化算法

## Adagrad

1. 独立设置参数空间每个轴方向上的学习率
2. 参数空间每个方向的学习率反比于 x 的平方根。x=该方向上梯度分量的所有历史平方值之和。
   $$ \vec{r}\leftarrow\vec{r}+\hat{\vec{g}}\odot\hat{\vec{g}} $$
$$ \vec{\theta}\leftarrow\vec{\theta}-\frac{\epsilon}{\sqrt{\vec{r}}}\odot\hat{\vec{g}} $$

## RMSProp

1. 将 Adagrad 梯度累计策略修改为指数加权的移动平均

## AdaDelta

1. 将 Adagrad 梯度累计策略修改为只考虑过去窗口 window 内的梯度。
2. RMSProp 可以认为是一种软化的 AdaDelta，衰减速率 window=1/(1-ρ)。

## Adam

1. RMSProp 算法中，通过累计平方梯度（采用指数移动平均）来修正学习率。
2. 而 Adam 算法中，不仅采用同样的方式来修正学习率，还通过累计梯度（采用指数移动平均）来修正梯度。
