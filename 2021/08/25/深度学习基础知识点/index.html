<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/favicon/web-app-manifest-192x192.png?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/favicon/web-app-manifest-512x512.png?v=2.8.0" type="image/png" sizes="32x32"><meta name="description" content="未完成，随缘更新">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习基础知识点">
<meta property="og:url" content="https://hxlh50k.github.io/2021/08/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9/index.html">
<meta property="og:site_name" content="一个博客">
<meta property="og:description" content="未完成，随缘更新">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-08-24T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-30T13:54:10.758Z">
<meta property="article:author" content="hxlh50k">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary"><title>深度学习基础知识点 | 一个博客</title><link ref="canonical" href="https://hxlh50k.github.io/2021/08/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 8.0.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">一个博客</div><div class="header-banner-info__subtitle">逸一时，误一世</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">深度学习基础知识点</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-08-25</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2025-09-30</span></span></div></header><div class="post-body">
        <h1 id="优化算法"   >
          <a href="#优化算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1>
      
        <h2 id="GD"   >
          <a href="#GD" class="heading-link"><i class="fas fa-link"></i></a><a href="#GD" class="headerlink" title="GD"></a>GD</h2>
      <ol>
<li>确定优化模型的假设函数及损失函数。</li>
<li>初始化参数，随机选取取值范围内的任意数；</li>
<li>迭代操作：<ol>
<li>计算当前梯度</li>
<li>修改新的变量</li>
<li>计算朝最陡的下坡方向走一步</li>
<li>判断是否需要终止，如否，梯度更新</li>
<li>得到全局最优解或者接近全局最优解。</li>
</ol>
</li>
</ol>

        <h2 id="BGD-批量梯度下降法"   >
          <a href="#BGD-批量梯度下降法" class="heading-link"><i class="fas fa-link"></i></a><a href="#BGD-批量梯度下降法" class="headerlink" title="BGD 批量梯度下降法"></a>BGD 批量梯度下降法</h2>
      <ol>
<li>采用所有数据来梯度下降。</li>
<li>批量梯度下降法在样本量很大的时候，训练速度慢。</li>
</ol>

        <h2 id="SGD-随机梯度下降法"   >
          <a href="#SGD-随机梯度下降法" class="heading-link"><i class="fas fa-link"></i></a><a href="#SGD-随机梯度下降法" class="headerlink" title="SGD 随机梯度下降法"></a>SGD 随机梯度下降法</h2>
      <ol>
<li>随机梯度下降用一个样本来梯度下降。</li>
<li>训练速度很快。</li>
<li>随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。</li>
<li>收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</li>
</ol>

        <h2 id="MBGD-小批量梯度下降"   >
          <a href="#MBGD-小批量梯度下降" class="heading-link"><i class="fas fa-link"></i></a><a href="#MBGD-小批量梯度下降" class="headerlink" title="MBGD 小批量梯度下降"></a>MBGD 小批量梯度下降</h2>
      <p>前两者折中</p>

        <h2 id="动量优化算法"   >
          <a href="#动量优化算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#动量优化算法" class="headerlink" title="动量优化算法"></a>动量优化算法</h2>
      <ul>
<li>GD 缺点：当接近最优值时梯度会比较小，由于学习率固定，收敛速度会变慢，有时甚至陷入局部最优。</li>
<li>动量基本思想：考虑历史梯度，引导参数朝着最优值更快收敛。直观上来说，要是当前时刻的梯度与历史梯度方向趋近，这种趋势会在当前时刻加强，否则当前时刻的梯度方向减弱。如果两个坐标轴梯度差距大，会在大梯度的轴上震荡</li>
</ul>

        <h2 id="牛顿动量（Nesterov）算法"   >
          <a href="#牛顿动量（Nesterov）算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#牛顿动量（Nesterov）算法" class="headerlink" title="牛顿动量（Nesterov）算法"></a>牛顿动量（Nesterov）算法</h2>
      <p>不在当前时刻修正梯度，而是预测下一次的位置，进行梯度修正。可以减弱震荡</p>

        <h1 id="自适应学习率优化算法"   >
          <a href="#自适应学习率优化算法" class="heading-link"><i class="fas fa-link"></i></a><a href="#自适应学习率优化算法" class="headerlink" title="自适应学习率优化算法"></a>自适应学习率优化算法</h1>
      
        <h2 id="Adagrad"   >
          <a href="#Adagrad" class="heading-link"><i class="fas fa-link"></i></a><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2>
      <ol>
<li>独立设置参数空间每个轴方向上的学习率</li>
<li>参数空间每个方向的学习率反比于 x 的平方根。x&#x3D;该方向上梯度分量的所有历史平方值之和。<br>$$ \vec{r}\leftarrow\vec{r}+\hat{\vec{g}}\odot\hat{\vec{g}} $$<br>$$ \vec{\theta}\leftarrow\vec{\theta}-\frac{\epsilon}{\sqrt{\vec{r}}}\odot\hat{\vec{g}} $$</li>
</ol>

        <h2 id="RMSProp"   >
          <a href="#RMSProp" class="heading-link"><i class="fas fa-link"></i></a><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2>
      <ol>
<li>将 Adagrad 梯度累计策略修改为指数加权的移动平均<br>$$ \vec{r}\leftarrow\rho\vec{r}+(1-\rho)\hat{\vec{g}}\odot\hat{\vec{g}} $$<br>$$ \vec{\theta}\leftarrow\vec{\theta}-\frac{\epsilon}{\sqrt{\vec{r}}}\odot\hat{\vec{g}} $$</li>
</ol>

        <h2 id="AdaDelta"   >
          <a href="#AdaDelta" class="heading-link"><i class="fas fa-link"></i></a><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h2>
      <ol>
<li>将 Adagrad 梯度累计策略修改为只考虑过去窗口 window 内的梯度。</li>
<li>RMSProp 可以认为是一种软化的 AdaDelta，衰减速率 $$window&#x3D;\frac{1}{1-ρ}$$。</li>
</ol>

        <h2 id="Adam"   >
          <a href="#Adam" class="heading-link"><i class="fas fa-link"></i></a><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2>
      <ol>
<li>RMSProp 算法中，通过累计平方梯度（采用指数移动平均）来修正学习率。</li>
<li>而 Adam 算法中，不仅采用同样的方式来修正学习率，还通过累计梯度（采用指数移动平均）来修正梯度。</li>
</ol>

        <h1 id="batch-size-如何设置"   >
          <a href="#batch-size-如何设置" class="heading-link"><i class="fas fa-link"></i></a><a href="#batch-size-如何设置" class="headerlink" title="batch-size 如何设置"></a>batch-size 如何设置</h1>
      <p>过小：花费时间多，同时梯度震荡严重，不利于收敛过大：不同 batch 的梯度方向没有任何变化，容易陷入局部极小值</p>

        <h1 id="正则化："   >
          <a href="#正则化：" class="heading-link"><i class="fas fa-link"></i></a><a href="#正则化：" class="headerlink" title="正则化："></a>正则化：</h1>
      <!-- prettier-ignore-start -->
<ul>
<li>正则化(Regularization) 是机器学习中对原始损失函数引入额外信息，以便防止过拟合和提高模型泛化性能的一类方法的统称。也就是目标函数变成了原始损失函数+额外项，常用的额外项一般有两种，英文称作 ℓ1−norm 和 ℓ2−norm，中文称作 L1 正则化和 L2 正则化，或者 L1 范数和 L2 范数（实际是 L2 范数的平方）。</li>
<li>L1 正则化和 L2 正则化可以看做是损失函数的惩罚项。所谓惩罚是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用 L1 正则化的模型叫做 Lasso 回归，使用 L2 正则化的模型叫做 Ridge 回归（岭回归）。</li>
<li>线性回归L1正则化损失函数：<br>$$ \min_w[\sum_{i&#x3D;1}^{N}{(w^Tx_i-y_i)^2+\lambda |w|_1}] $$  </li>
<li>线性回归L2正则化损失函数：<br>$$ \min_w[\sum_{i&#x3D;1}^{N}{(w^Tx_i-y_i)^2+\lambda |w|_2^2}] $$  </li>
<li>公式(1)(2)中w表示特征的系数（x的参数），可以看到正则化项是对系数做了限制。L1正则化和L2正则化的说明如下：  <ul>
<li>L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为$$ \left|w\right|_1 $$.</li>
<li>L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为$$ \left|w\right|^2_2 $$.</li>
<li>一般都会在正则化项之前添加一个系数$$\lambda$$。Python中用α表示，这个系数需要用户指定（也就是我们要调的超参）。</li>
</ul>
</li>
<li>L1 的作用是为了矩阵稀疏化。假设的是模型的参数取值满足拉普拉斯分布。</li>
<li>L2 的作用是为了使模型更平滑，可以获得更小的参数，得到更好的抗扰动能力。假设的是参数是满足高斯分布，L2 对大数和异常值更敏感。</li>
</ul>
<!-- prettier-ignore-end -->

        <h1 id="常见损失函数"   >
          <a href="#常见损失函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#常见损失函数" class="headerlink" title="常见损失函数"></a>常见损失函数</h1>
      <!-- prettier-ignore-start -->

        <h2 id="BCE"   >
          <a href="#BCE" class="heading-link"><i class="fas fa-link"></i></a><a href="#BCE" class="headerlink" title="BCE"></a>BCE</h2>
      <p>$$ L_{BCE}&#x3D;-\frac{1}{N}\sum^N_{i&#x3D;1}{y_i\log{p_i}+(1-y_i)\log{(1-p_i)}} $$<br>$$y_i$$ 的取值为 $${0, 1}$$ ，对于每个sample，公式中只有一项生效</p>

        <h2 id="CE"   >
          <a href="#CE" class="heading-link"><i class="fas fa-link"></i></a><a href="#CE" class="headerlink" title="CE"></a>CE</h2>
      <p>$$ L_{CE}&#x3D;-\frac{1}{N}\sum^N_{i&#x3D;1}{\sum^C_{c&#x3D;1}{y^{(c)}_i\log{p^{(c)}_i}}} $$</p>

        <h2 id="MAE-L1"   >
          <a href="#MAE-L1" class="heading-link"><i class="fas fa-link"></i></a><a href="#MAE-L1" class="headerlink" title="MAE&#x2F;L1"></a>MAE&#x2F;L1</h2>
      <p>$$ L_1 &#x3D; \frac{1}{N}\sum^N_{i&#x3D;1}{|y_i-p_i|} $$</p>

        <h2 id="Smooth-L1"   >
          <a href="#Smooth-L1" class="heading-link"><i class="fas fa-link"></i></a><a href="#Smooth-L1" class="headerlink" title="Smooth L1"></a>Smooth L1</h2>
      <p>$$ L_{s1} &#x3D; \begin{cases}<br>0.5x^2 &amp;, &amp; |x|&lt;1 \<br>|x|-0.5 &amp;, &amp; |x|\geq1 &amp;<br>\end{cases}$$</p>

        <h2 id="MSE-L2"   >
          <a href="#MSE-L2" class="heading-link"><i class="fas fa-link"></i></a><a href="#MSE-L2" class="headerlink" title="MSE&#x2F;L2"></a>MSE&#x2F;L2</h2>
      <p>$$ L_1 &#x3D; \frac{1}{N}\sum^N_{i&#x3D;1}{(y_i-p_i)^2} $$</p>

        <h2 id="FOCAL"   >
          <a href="#FOCAL" class="heading-link"><i class="fas fa-link"></i></a><a href="#FOCAL" class="headerlink" title="FOCAL"></a>FOCAL</h2>
      <p>$$ L_{FOCAL} &#x3D; \frac{1}{N}\sum^N_{i&#x3D;1}{\alpha(1-p_i)^\gamma y_i\log{p_i}+(1-\alpha)p_i^\gamma (1-y_i)\log(1-p_i)} , \alpha&#x3D;0.25,\gamma&#x3D;2$$</p>

        <h2 id="DICE"   >
          <a href="#DICE" class="heading-link"><i class="fas fa-link"></i></a><a href="#DICE" class="headerlink" title="DICE"></a>DICE</h2>
      <p>$$ L_{Dice} &#x3D; 1- 2*\frac{|y\cap p|}{|y|+|p|} $$</p>

        <h2 id="IOU"   >
          <a href="#IOU" class="heading-link"><i class="fas fa-link"></i></a><a href="#IOU" class="headerlink" title="IOU"></a>IOU</h2>
      <p>$$ L_{IOU} &#x3D; 1-\frac{|y \cap p|}{|y \cup p|} $$</p>

        <h1 id="激活函数"   >
          <a href="#激活函数" class="heading-link"><i class="fas fa-link"></i></a><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1>
      
        <h2 id="sigmoid"   >
          <a href="#sigmoid" class="heading-link"><i class="fas fa-link"></i></a><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h2>
      <p>$$ f(x) &#x3D; \frac{1}{1+e^{-x}} $$<br>$$ f’(x) &#x3D; f(x)(1-f(x)) $$<br>问题：梯度消失，均值不为0  </p>

        <h2 id="hard-sigmoid"   >
          <a href="#hard-sigmoid" class="heading-link"><i class="fas fa-link"></i></a><a href="#hard-sigmoid" class="headerlink" title="hard sigmoid"></a>hard sigmoid</h2>
      <p>$$ f(x) &#x3D; \begin{cases}<br>0 &amp;,&amp;{x&lt;-2.5}   \<br>0.2x-2.5 &amp;,&amp;{-2.5\leq x \leq 2.5}  \<br>1 &amp;,&amp;{x&gt;2.5}<br>\end{cases}$$</p>

        <h2 id="softmax"   >
          <a href="#softmax" class="heading-link"><i class="fas fa-link"></i></a><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2>
      <p>$$ f(x) &#x3D; \frac{1}{N}\sum^N_{i&#x3D;1}{\frac{e^{x_i}}{\sum^{C}_{c&#x3D;1}{e^{x_i^{(c)}}}}} $$</p>

        <h2 id="tanh"   >
          <a href="#tanh" class="heading-link"><i class="fas fa-link"></i></a><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h2>
      <p>$$ f(x) &#x3D; \frac{e^x-e^{-x}}{e^x+e^{-x}} $$<br>解决：均值为0<br>问题：梯度消失</p>

        <h2 id="ReLU"   >
          <a href="#ReLU" class="heading-link"><i class="fas fa-link"></i></a><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2>
      <p>$$ f(x)&#x3D;\max⁡(x,0) $$<br>解决：梯度消失<br>问题：死区，无穷大</p>

        <h2 id="ReLU6"   >
          <a href="#ReLU6" class="heading-link"><i class="fas fa-link"></i></a><a href="#ReLU6" class="headerlink" title="ReLU6"></a>ReLU6</h2>
      <p>$$ f(x)&#x3D;\min⁡(\max⁡(x,0),6)$$<br>解决：无穷大<br>问题：死区</p>

        <h2 id="Leaky-ReLU"   >
          <a href="#Leaky-ReLU" class="heading-link"><i class="fas fa-link"></i></a><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h2>
      <p>$$ f(x) &#x3D; \max(x,\alpha x),usually0&lt;\alpha \ll 1 $$<br>解决：死区<br>问题：α不好设置</p>

        <h2 id="PReLU"   >
          <a href="#PReLU" class="heading-link"><i class="fas fa-link"></i></a><a href="#PReLU" class="headerlink" title="PReLU"></a>PReLU</h2>
      <p>α可学习的Leaky ReLU，所有层相同</p>

        <h2 id="RReLU"   >
          <a href="#RReLU" class="heading-link"><i class="fas fa-link"></i></a><a href="#RReLU" class="headerlink" title="RReLU"></a>RReLU</h2>
      <p>α可学习的Leaky ReLU，随机均值&#x2F;高斯初始化，所有层不同</p>

        <h2 id="ELU"   >
          <a href="#ELU" class="heading-link"><i class="fas fa-link"></i></a><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2>
      <p>$$ f(x) &#x3D; \begin{cases}<br>x &amp;,&amp; x\geq 0 \<br>\lambda (e^{-x}-1) &amp;,&amp; x&lt;0<br>\end{cases} $$<br>缺点：计算量大</p>

        <h2 id="GeLU-for-bert"   >
          <a href="#GeLU-for-bert" class="heading-link"><i class="fas fa-link"></i></a><a href="#GeLU-for-bert" class="headerlink" title="GeLU (for bert)"></a>GeLU (for bert)</h2>
      <p>$$ xP(X\leq x)&#x3D;xΦ(x) $$<br>Φ(x)  is Standard normal distribution.</p>

        <h2 id="Maxout："   >
          <a href="#Maxout：" class="heading-link"><i class="fas fa-link"></i></a><a href="#Maxout：" class="headerlink" title="Maxout："></a>Maxout：</h2>
      <p>一种激活函数层</p>

        <h2 id="Swish"   >
          <a href="#Swish" class="heading-link"><i class="fas fa-link"></i></a><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h2>
      <p>$$ f(x)&#x3D;x\sigma(\beta x)$$<br>特点：</p>
<ul>
<li>无上界，不会梯度饱和</li>
<li>有下界，可以产生更强的正则化效果</li>
<li>非单调</li>
<li>处处可导</li>
</ul>

        <h2 id="H-Swish"   >
          <a href="#H-Swish" class="heading-link"><i class="fas fa-link"></i></a><a href="#H-Swish" class="headerlink" title="H-Swish"></a>H-Swish</h2>
      <p>$$ f(x) &#x3D; x\frac{ReLU6(x+3)}{6} $$</p>

        <h1 id="评价指标"   >
          <a href="#评价指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1>
      
        <h2 id="分类"   >
          <a href="#分类" class="heading-link"><i class="fas fa-link"></i></a><a href="#分类" class="headerlink" title="分类"></a>分类</h2>
      
        <h3 id="准确率"   >
          <a href="#准确率" class="heading-link"><i class="fas fa-link"></i></a><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h3>
      <p>$$ Accuracy &#x3D; \frac{TP+TN}{TP+FN+FP+TN}$$</p>

        <h3 id="精确率"   >
          <a href="#精确率" class="heading-link"><i class="fas fa-link"></i></a><a href="#精确率" class="headerlink" title="精确率"></a>精确率</h3>
      <p>$$ Precision &#x3D; \frac{TP}{TP+FP}$$</p>

        <h3 id="召回率"   >
          <a href="#召回率" class="heading-link"><i class="fas fa-link"></i></a><a href="#召回率" class="headerlink" title="召回率"></a>召回率</h3>
      <p>$$ Recall &#x3D; \frac{TP}{TP+FN} $$</p>

        <h3 id="F-score"   >
          <a href="#F-score" class="heading-link"><i class="fas fa-link"></i></a><a href="#F-score" class="headerlink" title="F-score"></a>F-score</h3>
      <p>$$ F_β &#x3D; \frac{(1+\beta^2 )TP}{(1+\beta^2 )TP+\beta^2 FN+FP}$$<br>当beta大于1，更多关注recall；当beta小于1，更多关注precision。</p>

        <h3 id="AUC-GAUC-https-zhuanlan-zhihu-com-p-84350940"   >
          <a href="#AUC-GAUC-https-zhuanlan-zhihu-com-p-84350940" class="heading-link"><i class="fas fa-link"></i></a><a href="#AUC-GAUC-https-zhuanlan-zhihu-com-p-84350940" class="headerlink" title="AUC&#x2F;GAUC https://zhuanlan.zhihu.com/p/84350940"></a>AUC&#x2F;GAUC <span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/84350940" >https://zhuanlan.zhihu.com/p/84350940</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></h3>
      
        <h4 id="ROC"   >
          <a href="#ROC" class="heading-link"><i class="fas fa-link"></i></a><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h4>
      <p>给定所有样本的正负和预测的概率值，给定阈值集合{0.1,0.2…0.9}，遍历所有阈值，并查看给定每个阈值的情况下的分类情况如何。横坐标TPR，纵坐标FPR</p>

        <h4 id="AUC"   >
          <a href="#AUC" class="heading-link"><i class="fas fa-link"></i></a><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h4>
      <p>$$ AUC &#x3D; \frac{area under ROC}{x<em>y} &#x3D; \frac{\sum^M_{i&#x3D;1}{N_i}}{M</em>N} $$<br>有M个正样本，N个负样本</p>

        <h4 id="GAUC"   >
          <a href="#GAUC" class="heading-link"><i class="fas fa-link"></i></a><a href="#GAUC" class="headerlink" title="GAUC"></a>GAUC</h4>
      <p>用于广告推荐领域，计算每个用户的auc，然后加权平均，最后得到group auc，这样就能减少不同用户间的排序结果不太好比较这一影响。</p>
<!-- prettier-ignore-end -->


        <h1 id="Normalization-层"   >
          <a href="#Normalization-层" class="heading-link"><i class="fas fa-link"></i></a><a href="#Normalization-层" class="headerlink" title="Normalization 层"></a>Normalization 层</h1>
      
        <h2 id="Normlization-层的作用"   >
          <a href="#Normlization-层的作用" class="heading-link"><i class="fas fa-link"></i></a><a href="#Normlization-层的作用" class="headerlink" title="Normlization 层的作用"></a>Normlization 层的作用</h2>
      <p>加快模型收敛速度，缓解深层网络中“梯度弥散”的问题；</p>

        <h2 id="BN、IN、GN、LN-区别"   >
          <a href="#BN、IN、GN、LN-区别" class="heading-link"><i class="fas fa-link"></i></a><a href="#BN、IN、GN、LN-区别" class="headerlink" title="BN、IN、GN、LN 区别"></a>BN、IN、GN、LN 区别</h2>
      <p>feature map: $$ x\in R^{N×C×H×W} $$</p>
<ul>
<li>BN：保留 C 求均值，受到 BS 的影响大</li>
<li>LN：保留 N 求均值，单样本可进行</li>
<li>IN：保留 N、C 求均值，单样本可进行，起初用于风格迁移</li>
<li>GN：LN、IN 的折中，将 C 分为 G 组</li>
</ul>

        <h2 id="BN-计算"   >
          <a href="#BN-计算" class="heading-link"><i class="fas fa-link"></i></a><a href="#BN-计算" class="headerlink" title="BN 计算"></a>BN 计算</h2>
      <!-- prettier-ignore-start -->
<p>$$ μ_j&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^m{Z_j^{(i)}}$$<br>$$ σ_j^2&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^m{(Z_j^{(i)}-\mu_j)^2} $$<br>$$ \hat{Z_j}&#x3D;\frac{Z_j-μ_j}{\sqrt{σ_j^2+\epsilon}} $$<br>$$ \tilde{Z_j}&#x3D;\gamma_j*\hat{Z_j}+\beta_j $$<br>inferenct阶段的 $$\mu\ \sigma$$ 使用 train 阶段的无偏估计</p>
<!-- prettier-ignore-end -->

<ul>
<li>深层神经网络在做非线性变换前的激活输入值($$x&#x3D;WU+B$$)随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于 Sigmoid 函数来说，意味着激活输入值 $$WU+B$$ 是大的负值或正值），导致后向传播时低层神经网络的梯度消失。BN 就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为 0 方差为 1 的标准正态分布，使得激活输入值落在非线性函数对输入比较敏感的区域。</li>
<li>归一化后会降低表征能力，BN 为了保证非线性的获得，对变换后的满足均值为 0 方差为 1 的 x 又进行了 scale 加上 shift 操作($$y&#x3D;scale×x+shift$$)，每个神经元增加了两个参数 scale 和 shift 参数，这两个参数是通过训练学习到的，意思是通过 scale 和 shift 把这个值从标准正态分布左移或者由移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。</li>
</ul>

        <h3 id="优点："   >
          <a href="#优点：" class="heading-link"><i class="fas fa-link"></i></a><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3>
      <ul>
<li>BN 将 Hidden Layer 的输入分布从饱和区拉到了非饱和区，减小了梯度弥散，提升了训练速度，收敛过程大大加快，还能增加分类效果。</li>
<li>BatchNorm 本身上也是一种正则的方式（主要缓解了梯度消失），可以代替其他正则方式如 dropout 等。</li>
<li>调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。</li>
</ul>

        <h4 id="缺点："   >
          <a href="#缺点：" class="heading-link"><i class="fas fa-link"></i></a><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4>
      <ul>
<li>batch normalization 依赖于 batch 的大小，当 batch 值很小时，计算的均值和方差不稳定。</li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://hxlh50k.github.io">hxlh50k</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://hxlh50k.github.io/2021/08/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9/">https://hxlh50k.github.io/2021/08/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://hxlh50k.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://hxlh50k.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">机器学习知识点</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/08/05/Docker%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"><span class="paginator-prev__text">Docker快速入门</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">
          优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#GD"><span class="toc-number">1.1.</span> <span class="toc-text">
          GD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BGD-%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">
          BGD 批量梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SGD-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">
          SGD 随机梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MBGD-%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.4.</span> <span class="toc-text">
          MBGD 小批量梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E9%87%8F%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">1.5.</span> <span class="toc-text">
          动量优化算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%9B%E9%A1%BF%E5%8A%A8%E9%87%8F%EF%BC%88Nesterov%EF%BC%89%E7%AE%97%E6%B3%95"><span class="toc-number">1.6.</span> <span class="toc-text">
          牛顿动量（Nesterov）算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">
          自适应学习率优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Adagrad"><span class="toc-number">2.1.</span> <span class="toc-text">
          Adagrad</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RMSProp"><span class="toc-number">2.2.</span> <span class="toc-text">
          RMSProp</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AdaDelta"><span class="toc-number">2.3.</span> <span class="toc-text">
          AdaDelta</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adam"><span class="toc-number">2.4.</span> <span class="toc-text">
          Adam</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#batch-size-%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE"><span class="toc-number">3.</span> <span class="toc-text">
          batch-size 如何设置</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%9A"><span class="toc-number">4.</span> <span class="toc-text">
          正则化：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.</span> <span class="toc-text">
          常见损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#BCE"><span class="toc-number">5.1.</span> <span class="toc-text">
          BCE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CE"><span class="toc-number">5.2.</span> <span class="toc-text">
          CE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MAE-L1"><span class="toc-number">5.3.</span> <span class="toc-text">
          MAE&#x2F;L1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Smooth-L1"><span class="toc-number">5.4.</span> <span class="toc-text">
          Smooth L1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MSE-L2"><span class="toc-number">5.5.</span> <span class="toc-text">
          MSE&#x2F;L2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FOCAL"><span class="toc-number">5.6.</span> <span class="toc-text">
          FOCAL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DICE"><span class="toc-number">5.7.</span> <span class="toc-text">
          DICE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IOU"><span class="toc-number">5.8.</span> <span class="toc-text">
          IOU</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">6.</span> <span class="toc-text">
          激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sigmoid"><span class="toc-number">6.1.</span> <span class="toc-text">
          sigmoid</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hard-sigmoid"><span class="toc-number">6.2.</span> <span class="toc-text">
          hard sigmoid</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax"><span class="toc-number">6.3.</span> <span class="toc-text">
          softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tanh"><span class="toc-number">6.4.</span> <span class="toc-text">
          tanh</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ReLU"><span class="toc-number">6.5.</span> <span class="toc-text">
          ReLU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ReLU6"><span class="toc-number">6.6.</span> <span class="toc-text">
          ReLU6</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Leaky-ReLU"><span class="toc-number">6.7.</span> <span class="toc-text">
          Leaky ReLU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PReLU"><span class="toc-number">6.8.</span> <span class="toc-text">
          PReLU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RReLU"><span class="toc-number">6.9.</span> <span class="toc-text">
          RReLU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ELU"><span class="toc-number">6.10.</span> <span class="toc-text">
          ELU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GeLU-for-bert"><span class="toc-number">6.11.</span> <span class="toc-text">
          GeLU (for bert)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Maxout%EF%BC%9A"><span class="toc-number">6.12.</span> <span class="toc-text">
          Maxout：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Swish"><span class="toc-number">6.13.</span> <span class="toc-text">
          Swish</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#H-Swish"><span class="toc-number">6.14.</span> <span class="toc-text">
          H-Swish</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">7.</span> <span class="toc-text">
          评价指标</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB"><span class="toc-number">7.1.</span> <span class="toc-text">
          分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87"><span class="toc-number">7.1.1.</span> <span class="toc-text">
          准确率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B2%BE%E7%A1%AE%E7%8E%87"><span class="toc-number">7.1.2.</span> <span class="toc-text">
          精确率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AC%E5%9B%9E%E7%8E%87"><span class="toc-number">7.1.3.</span> <span class="toc-text">
          召回率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#F-score"><span class="toc-number">7.1.4.</span> <span class="toc-text">
          F-score</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AUC-GAUC-https-zhuanlan-zhihu-com-p-84350940"><span class="toc-number">7.1.5.</span> <span class="toc-text">
          AUC&#x2F;GAUC https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;84350940</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ROC"><span class="toc-number">7.1.5.1.</span> <span class="toc-text">
          ROC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AUC"><span class="toc-number">7.1.5.2.</span> <span class="toc-text">
          AUC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GAUC"><span class="toc-number">7.1.5.3.</span> <span class="toc-text">
          GAUC</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Normalization-%E5%B1%82"><span class="toc-number">8.</span> <span class="toc-text">
          Normalization 层</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Normlization-%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">8.1.</span> <span class="toc-text">
          Normlization 层的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BN%E3%80%81IN%E3%80%81GN%E3%80%81LN-%E5%8C%BA%E5%88%AB"><span class="toc-number">8.2.</span> <span class="toc-text">
          BN、IN、GN、LN 区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BN-%E8%AE%A1%E7%AE%97"><span class="toc-number">8.3.</span> <span class="toc-text">
          BN 计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9%EF%BC%9A"><span class="toc-number">8.3.1.</span> <span class="toc-text">
          优点：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A"><span class="toc-number">8.3.1.1.</span> <span class="toc-text">
          缺点：</span></a></li></ol></li></ol></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/avatar/avatar-512x512.png" alt="avatar"></div><p class="sidebar-ov-author__text">hxlh50k</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">17</div><div class="sidebar-ov-state-item__name">归档</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2025</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>hxlh50k</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v8.0.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.8.0</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>